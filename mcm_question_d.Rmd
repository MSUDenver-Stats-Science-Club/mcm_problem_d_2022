---
title: "ICM Report"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    includes:
      in_header: "headers.tex"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(flextable)

flextable::set_flextable_defaults(
  fonts_ignore = TRUE
)

```

# Intercontinental Cargo Moving Corporation Evaluation

## Background

Intercontinental Cargo Moving Corporation (ICM) operates a large seaport and has hired our team for data and analytics (DA) system evaluation. ICM wants us to measure the maturity of their current DA system and provide a solid plan to optimize their DA capabilities. Using our company's model, ICM hopes to install customer trust and confidence in their data practices. We can strengthen customer trust and confidence by supplying ICM with tools to more effectively manage their people, technologies, and processes.

To provide ICM with the aforementioned tools, our team is tasked with implementing a model and providing a report that will provide:

  - Metrics to measure the current DA system maturity level. This should be a calculation of KPIs used to measure the success of their people, technologies, and processes.
  - Demonstrations of the model's application and how results from the model could be used to recommend future changes to the system allowing for the company to maximize the potential of their data assets.
  - Suggest protocols that ICM can enact to measure their system's effectiveness.
  - Demonstrate how the model can be applied to subsequent seaports of varying sizes. We can also analyze how this model can be adapted to other industries, how it could benefit clients of ICM, and how ICM can benefit by interacting with clients using this model.
  
## Model

### People



### Technology



### Process

#### Data Governance

Now that we've discussed methods to evaluate the previous components of ICM's DA system, we can dive into the processes that ICM should follow to ensure a strong data governance program. As mentioned within the problem guidelines, a good data governance program provides oversight of data resources, access levels, modifications to data sources and tables, and follows consistent protocols. Processes that encourage strong data governance are supported by the people within the department and the technologies employed by the company. In these situations, its required that ICM utilizes an ERP that helps manage data by providing a single source of truth.

To prescribe a plan of action to ICM's Information Security Officer (ISO), we first start by outlining proper structure and content of data sources to eliminate any inconsistencies between independent data sources derived from port operations, transportation schedules, customs inspections, and container storage. We want to make sure that operations data is maintained and easily accessible to our DA systems team when we need to gather insights from our activity. Additionally, its important to plan for company longevity since our data will grow considerably with company growth. As our data continues to increase with time, it should remain just as manageable and easily accessible by the DA systems team. In the event of an error or oversight the data collected and utilized by ICM should also follow protocols for data archival. Archiving operations data ensures that if something bad happens there is minimal downtime and data recovery is quick.

We can evaluate ICM's current program by measuring their current ability to access and gain consistent insights from their operations data:

  - It's given that independent data processes are creating inconsistencies between operations data sources. Because of the inconsistent data, reports that are derived from each data set will result in different figures leading to visibility of operations activity being impossible. This is an area that needs to be promptly addressed, otherwise management will lack the guidance needed to run the company successfully.
  - We're currently unaware of any protocols that are in practice by the DA systems team that would enforce strong internal controls relating to modifying and maintaining data sets among each area of operations. To implement a strong data governance system its important that the team follows proper protocols so that data is correctly modified, maintained, and archived. Tables should be used to communicate the activity within each area of operations so that managers can view a single status report with accountable figures.
  - There current process does not include protocols for data management throughout the data lifecycle. We can ensure the longevity of the data as the company grows by enacting processes that ensure we have the capacity to store and access big data generated by ICM.
  - Finally, we want to make sure that there is a clear structure for the DA systems team so that staff roles are clearly defined and individuals have the appropriate access levels. With data governance in mind, ICM should enforce a least-privileged system where data engineers are given the ability to maintain and modify data sources. Other staff roles such as data scientists, data analysts, and machine learning engineers should only have the ability to access the data for analysis. These permissions will help reduce unwanted or unforeseeable side-effects from inexperienced users, therefore reducing the possibility of downtime.
  
Prescriptions for a strong data governance program would be a direct response to pitfalls outlined in the previous assessment. We would strongly encourage ICM to generate data using the capabilities offered by their ERP and collect that subsequently generated data for analysis of operations. By accessing the data from a single source of truth and following strict data engineering processes to ensure that the data is maintained, data analysts and data scientists within the DA systems team can produce reports of accountable figures to management for visibility of operations system status. Additionally, strict protocols for access levels, and archival will provide minimal downtime for data recovery. Finally, we can suggest that data is maintained and flexible for longevity as the company continues to grow.

Within the following section we show how a system following a single source of truth can be used to track incoming and outgoing freight that the port will interact with while in operation. This data would live within properly managed data containers in data warehouses set up by the data engineering staff.

We create two tables that track imports and exports so that we can provide management with insights as to which containers are entering and exiting the port. This would allow us to track the number of containers we are currently holding, when they should be scheduled to leave the port, whether or not they should be going through customs, and the owner of the containers.

```{r import_export, echo=FALSE}
## `dim_imports` and `dim_exports` look at the import and export of items 
## within the port and provides details on those item's origins. 
dim_imports <- 
  dplyr::tibble(
    transaction_type = rep("Import", 3),
    client_id = "A-001",
    client_name = "Metal Manufacturers, Inc.",
    num_containers = 3,
    container_ids = c("45A-000371", "45A-000372", "45A-000373"),
    transaction_id = paste0(client_id, "-", container_ids),
    import_date = "2022-02-20",
    import_method = rep("Land", 3),
    schd_export_date = "2022-02-26",
    customs_check = rep(FALSE, 3),
    customs_check_date = ifelse(
      isFALSE(customs_check),
      NA_character_, 
      as.character(Sys.Date())
    )
  ) %>%
  dplyr::relocate(transaction_id)

dim_exports <-
  dplyr::tibble(
    transaction_type = rep("Export", 2),
    client_id = "A-001",
    client_name = "Metal Manufacturers, Inc.",
    num_containers = 2,
    container_ids = c("45A-000371", "45A-000372"),
    export_datetime = "2022-02-26",
    export_method = rep("Sea", 2),
    transaction_id = paste0(client_id, "-", container_ids)
  ) %>%
  dplyr::relocate(transaction_id)

dim_imports_ft <- flextable::flextable(dim_imports) %>%
  flextable::add_footer_lines("An example of import data that could be contained within the appropriate database container.") %>%
  flextable::set_caption("Import Dimensional Table") %>%
  flextable::theme_vanilla() %>%
  flextable::set_table_properties(layout = "autofit")

dim_exports_ft <- flextable::flextable(dim_exports) %>%
  flextable::add_footer_lines("An example of export data that could be contained within the appropriate database container.") %>%
  flextable::set_caption("Export Dimensional Table") %>%
  flextable::theme_vanilla() %>%
  flextable::set_table_properties(layout = "autofit")

dim_imports_ft
dim_exports_ft

```

Similarly, we can audit where specific containers are transported after we receive them. Using a dimensional table that tracks where containers are relocated after we receive them, or before and after going through the customs process would be a useful tool to audit which containers we still have in our possession and where. To illustrate this idea, we include a matrix of hypothetical grid locations within the port used for storing these containers.

```{r transport}
## `dim_port_transportation` looks at the transportation schedules of the items within the port.
## storage_location should be a function of schd_export_date so that we're storing items in the appropriate location within the port.
v <- c(paste0("A", 1:6), paste0("B", 1:6), paste0("C", 1:6)) %>%
  matrix(., ncol = 3, byrow = TRUE)

dim_port_transportation <-
  dplyr::tibble(
    transaction_type = "Relocate",
    client_id = "A-001",
    client_name = "Metal Manufacturers, Inc.",
    num_containers = 3,
    container_ids = c("45A-000371", "45A-000372", "45A-000373"),
    transport_date = "2022-02-20",
    transport_from = "A1",
    transport_to = "B3"
  ) %>%
  dplyr::add_row(
    transaction_type = "Relocate",
    client_id = "A-001",
    client_name = "Metal Manufacturers, Inc.",
    num_containers = 2,
    container_ids = c("45A-000371", "45A-000372"),
    transport_date = "2022-02-26",
    transport_from = "B3",
    transport_to = "A1"
  ) %>%
  dplyr::mutate(
    transaction_id = paste0(client_id, "-", container_ids)
  ) %>%
  dplyr::relocate(transaction_id)



```

```{r operations}

## `fact_operations` allows managers to gain visibility into the daily operations occuring within the port.
fact_operations <- 
  dim_imports %>%
  dplyr::full_join(
    dim_exports,
    by = c(
      "transaction_id",
      "transaction_type",
      "client_id",
      "client_name",
      "num_containers",
      "container_ids"
    )
  ) %>%
  dplyr::full_join(
    dim_port_transportation,
    by = c(
      "transaction_id",
      "transaction_type",
      "client_id",
      "client_name",
      "num_containers",
      "container_ids"
    )
  )

```

#### Metadata

Metadata is equally as important as some of the items outlined in the previous data governance section. To reduce the time spent by members of the DA system team investigating the use and contents of data sets, we can provide metadata that explains the use and contents of the data. ICM can create effective metadata by drafting documentation that lists each table contained within a database, their uses, and which area of the business they pertain to. Within each table of the database, ICM should create a list of all columns of that table, the data types of each column, the primary keys associated with that table, and the contents that each column represents. Strong documentation can assist with the longevity and archival of data as tables outlive their usefulness, are superseded or deprecated, or become irrelevant if the data contained within the table is no longer needed for analysis. 

## Letter to Interested Parties

